<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>fb102bf6427044d1bc72799cd97b023c</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div class="cell code" data-execution_count="1">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.mixture <span class="im">import</span> GaussianMixture</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> ReLU</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> Linear</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> trange, tqdm</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="2">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AutoEncoder(torch.nn.Module):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, bottleneck_size, hidden_size, layers) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(AutoEncoder, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span>(hidden_size <span class="op">&gt;</span> input_size)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_size <span class="op">=</span> input_size</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bottleneck_size <span class="op">=</span> bottleneck_size</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> layers</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># encoder</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># input layer</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        layer_list <span class="op">=</span> [Linear(input_size, hidden_size), ReLU()]</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden layers</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(layers):</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>            layer_list.append(Linear(hidden_size, hidden_size))</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>            layer_list.append(ReLU())</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># output layers</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        layer_list.append(Linear(hidden_size, bottleneck_size))</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> torch.nn.Sequential(<span class="op">*</span>layer_list)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># decoder</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># input layer</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        layer_list <span class="op">=</span> [Linear(bottleneck_size, hidden_size), ReLU()]</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># hidden layers</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(layers):</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>            layer_list.append(Linear(hidden_size, hidden_size))</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>            layer_list.append(ReLU())</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># output layers</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        layer_list.append(Linear(hidden_size, input_size))</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> torch.nn.Sequential(<span class="op">*</span>layer_list)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, x):</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, z):</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.decoder(z)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.decoder(<span class="va">self</span>.encoder(x))</span></code></pre></div>
</div>
<div class="cell markdown">
<div style="color: green; font-weight:
bold"> Comments:  

- Ensure the network depth is consistent with the task's requirements. The code creates a deeper network than the sample's, which might not be necessary and could lead to overfitting or increased computational cost.</div>
</div>
<div class="cell code" data-execution_count="3">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># hyperparameters: training set size, number epochs, learning rate</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_autoencoder(model, X_train, epochs<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">0.001</span>, loss<span class="op">=</span>torch.nn.MSELoss()):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    optim <span class="op">=</span> torch.optim.Adam(model.parameters(), lr)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(X_train)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> loss(output, X_train)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        model.zero_grad()</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        error.backward()</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        optim.step()</span></code></pre></div>
</div>
<section id="evaluation" class="cell markdown">
<h1>Evaluation</h1>
</section>
<div class="cell code" data-execution_count="4">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>training_parameters <span class="op">=</span> [</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1000</span>, <span class="dv">3000</span>, <span class="dv">5000</span>, <span class="dv">7000</span>, <span class="dv">9000</span>],</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">300</span>, <span class="dv">500</span>, <span class="dv">1000</span>],</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.003</span>, <span class="fl">0.002</span>, <span class="fl">0.001</span>, <span class="fl">0.0005</span>]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_hyperparameters(X_train, X_test, bottleneck_size, hidden_size, layers, training_parameters):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    N, input_size <span class="op">=</span> X_train.shape</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create data array</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    errors <span class="op">=</span> np.zeros((<span class="bu">len</span>(training_parameters[<span class="dv">0</span>]), <span class="bu">len</span>(training_parameters[<span class="dv">1</span>]), <span class="bu">len</span>(training_parameters[<span class="dv">2</span>])))</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, s <span class="kw">in</span> tqdm(<span class="bu">enumerate</span>(training_parameters[<span class="dv">0</span>]), total<span class="op">=</span><span class="bu">len</span>(training_parameters[<span class="dv">0</span>])):</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j, e <span class="kw">in</span> <span class="bu">enumerate</span>(training_parameters[<span class="dv">1</span>]):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k, l <span class="kw">in</span> <span class="bu">enumerate</span>(training_parameters[<span class="dv">2</span>]):</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>                model <span class="op">=</span> AutoEncoder(input_size, bottleneck_size, hidden_size, layers)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>                train_autoencoder(model, X_train[:s], e, l)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>                output <span class="op">=</span> model(X_test)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>                errors[i, j, k] <span class="op">=</span> torch.nn.MSELoss()(X_test, output)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> errors</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_line(title, x, y, x_label, y_label):</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, y)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(x_label)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(y_label)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_list_of_numbers(title, images):</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    full_img <span class="op">=</span> np.hstack([img.reshape(<span class="dv">8</span>, <span class="dv">8</span>) <span class="cf">for</span> img <span class="kw">in</span> images])</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    plt.imshow(full_img)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_reconstructed_point(title, point, reconst_point):</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    points <span class="op">=</span> np.array([point, reconst_point])</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    plt.scatter(points[:, <span class="dv">0</span>], points[:, <span class="dv">1</span>])</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>possible_parameters<span class="op">=</span> [<span class="st">&#39;size&#39;</span>, <span class="st">&#39;epochs&#39;</span>, <span class="st">&#39;lr&#39;</span>]</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_min_for_parameter(test_values, errors, parameter<span class="op">=</span><span class="st">&quot;&quot;</span>):</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(parameter <span class="kw">in</span> possible_parameters)</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> parameter <span class="op">==</span> <span class="st">&#39;size&#39;</span>:</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>        plot_line(<span class="st">&quot;Minimum Training Error - Training Sizes&quot;</span>, test_values, np.<span class="bu">min</span>(errors, axis<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>)), <span class="st">&quot;Sizes&quot;</span>, <span class="st">&quot;Error&quot;</span>)</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> parameter <span class="op">==</span> <span class="st">&#39;epochs&#39;</span>:</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>        plot_line(<span class="st">&quot;Minimum Training Error - Epochs&quot;</span>, test_values, np.<span class="bu">min</span>(errors, axis<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>)), <span class="st">&quot;Epochs&quot;</span>, <span class="st">&quot;Error&quot;</span>)</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> parameter <span class="op">==</span> <span class="st">&#39;lr&#39;</span>:</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>        plot_line(<span class="st">&quot;Minimum Training Error - Learning Rate&quot;</span>, test_values, np.<span class="bu">min</span>(errors, axis<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>)), <span class="st">&quot;Learning Rate&quot;</span>, <span class="st">&quot;Error&quot;</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="5">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>X_train_moon, _ <span class="op">=</span> make_moons(<span class="dv">10000</span>, noise<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>X_train_moon <span class="op">=</span> torch.from_numpy(X_train_moon).<span class="bu">type</span>(torch.float32)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>X_test_moon, _ <span class="op">=</span> make_moons(<span class="dv">10000</span>, noise<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>X_test_moon <span class="op">=</span> torch.from_numpy(X_test_moon).<span class="bu">type</span>(torch.float32)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>errors <span class="op">=</span> test_hyperparameters(X_train_moon, X_test_moon, <span class="dv">1</span>, <span class="dv">20</span>, <span class="dv">3</span>, training_parameters)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>best_training_size <span class="op">=</span> training_parameters[<span class="dv">0</span>][np.argmin(np.<span class="bu">min</span>(errors, axis<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>)))]</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>best_epochs <span class="op">=</span> training_parameters[<span class="dv">1</span>][np.argmin(np.<span class="bu">min</span>(errors, axis<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>)))]</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>best_lr <span class="op">=</span> training_parameters[<span class="dv">2</span>][np.argmin(np.<span class="bu">min</span>(errors, axis<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>)))]</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;best combination: size = </span><span class="sc">{</span>best_training_size<span class="sc">}</span><span class="ss">, epochs = </span><span class="sc">{</span>best_epochs<span class="sc">}</span><span class="ss">, lr = </span><span class="sc">{</span>best_lr<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(possible_parameters):</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    plot_min_for_parameter(training_parameters[i], errors, p)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<div class="output stream stderr">
<pre><code>100%|██████████| 5/5 [03:50&lt;00:00, 46.17s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>best combination: size = 9000, epochs = 1000, lr = 0.002
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/b581bf4a4fb1c499817a8e643d71ee67a38b901c.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/db9e892f11f521610b2e29b2b65ed9698193059f.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/48e1eafd4982b375236bcb8f3ee34d798d96d3c1.png" /></p>
</div>
</div>
<div class="cell markdown">
<h3 id="training-size">Training Size</h3>
<p>We observe the best result for the most instances, however there is
an increase in the middle, probably due to a bad initialization.</p>
<h3 id="epochs">Epochs</h3>
<p>We see a monotonic decrease in the error as expected.</p>
<h3 id="learning-rate">Learning Rate</h3>
<p>It seems as if a sweet spot is around 0.002, but for some runs we
also had 0.003</p>
</div>
<div class="cell code" data-execution_count="6">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>hidden_sizes <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>, <span class="dv">40</span>, <span class="dv">50</span>]</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>layer_sizes <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_network_structure(X_train, X_test, bottleneck_size, hidden_sizes, layer_sizes, epochs, lr):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    input_size <span class="op">=</span> X_train.shape[<span class="dv">1</span>]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create data array</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    errors <span class="op">=</span> np.zeros((<span class="bu">len</span>(hidden_sizes), <span class="bu">len</span>(layer_sizes)))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, h <span class="kw">in</span> tqdm(<span class="bu">enumerate</span>(hidden_sizes), total<span class="op">=</span><span class="bu">len</span>(hidden_sizes)):</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j, l <span class="kw">in</span> <span class="bu">enumerate</span>(layer_sizes):</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> AutoEncoder(input_size, bottleneck_size, h, l)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>            train_autoencoder(model, X_train, epochs, lr)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> model(X_test)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>            errors[i, j] <span class="op">=</span> torch.nn.MSELoss()(X_test, output)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> errors</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>errors <span class="op">=</span> test_network_structure(X_train_moon, X_test_moon, <span class="dv">1</span>, hidden_sizes, layer_sizes, best_epochs, best_lr)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>best_hidden_size <span class="op">=</span> hidden_sizes[np.argmin(np.<span class="bu">min</span>(errors, axis<span class="op">=</span><span class="dv">1</span>))]</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>best_layer_count <span class="op">=</span> layer_sizes[np.argmin(np.<span class="bu">min</span>(errors, axis<span class="op">=</span><span class="dv">0</span>))]</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;best network: hidden: </span><span class="sc">{</span>best_hidden_size<span class="sc">}</span><span class="ss">, layer count: </span><span class="sc">{</span>best_layer_count<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>100%|██████████| 5/5 [03:04&lt;00:00, 36.95s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>best network: hidden: 50, layer count: 4
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
</code></pre>
</div>
</div>
<section id="original-vs-reconstructions" class="cell markdown">
<h2>Original vs. Reconstructions</h2>
</section>
<div class="cell code" data-execution_count="7">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>best_model_moons <span class="op">=</span> AutoEncoder(<span class="dv">2</span>, <span class="dv">1</span>, best_hidden_size, best_layer_count)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>train_autoencoder(best_model_moons, X_train_moon[:best_training_size], best_epochs, best_lr)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> best_model_moons(X_test_moon)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> plt.scatter(X_test_moon[:, <span class="dv">0</span>], X_test_moon[:, <span class="dv">1</span>])</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> plt.scatter(out[:, <span class="dv">0</span>].detach().numpy(), out[:, <span class="dv">1</span>].detach().numpy())</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Original Points vs Reconstruction with best parameters&quot;</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>plt.legend([a, b],[<span class="st">&quot;Original Data&quot;</span>, <span class="st">&quot;Reconstructed Data&quot;</span>])</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/9df17aa954d9e564f73c3dc9140777c06da69d84.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>As expected of a bottleneck of 1, the reconstructions follow a line.
The reconstruction line lies in the middle of the two moons with a
connection, that is sampled although there are no original points
between the moons.</p>
</div>
<div class="cell markdown">
<div style="color: green; font-weight:
bold"> <h1>Comments:  </h1>
<ol>
<li>Implementing batch training can enhance the efficiency of the training process, especially for larger datasets.<br>  </li>

<li>Incorporating tools like TensorBoard can provide more insightful visualizations and real-time monitoring of the training process.<br>  </li>

<li>The nested loop structure for testing hyperparameters, while functional, can be optimized further, potentially using more advanced techniques like grid search or random search.<br> </li> 

<li>More detailed comments on the implications of the findings, especially how they relate to the two moons dataset's specific characteristics, would be valuable.<br>  </li>

<li>The analysis of the "sweet spot" for learning rates and the effect of epochs on error reduction shows a good understanding of model training techniques.</div>
</li>
</ol>
</div>
<section
id="repeated-training-with-best-parameters-and-variance-in-errors"
class="cell markdown">
<h2>Repeated training with best parameters and variance in errors</h2>
</section>
<div class="cell code" data-execution_count="8">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># same training data</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>errors_same <span class="op">=</span> []</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> trange(<span class="dv">2</span>):</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> AutoEncoder(<span class="dv">2</span>, <span class="dv">1</span>, best_hidden_size, best_layer_count)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    train_autoencoder(model, X_train_moon[:best_training_size], best_epochs, best_lr)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    errors_same.append(torch.nn.MSELoss()(X_test_moon, model(X_test_moon)).detach())</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># varying training data</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>errors_var <span class="op">=</span> []</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> trange(<span class="dv">2</span>):</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    X_var, _ <span class="op">=</span> make_moons(<span class="dv">10000</span>, noise<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    X_var <span class="op">=</span> torch.from_numpy(X_var).<span class="bu">type</span>(torch.float32)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> AutoEncoder(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">2</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    train_autoencoder(model, X_var[:best_training_size], best_epochs, best_lr)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    errors_var.append(torch.nn.MSELoss()(X_test_moon, model(X_test_moon)).detach())</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    mean_same <span class="op">=</span> np.mean(errors_same)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>std_same <span class="op">=</span> np.std(errors_same)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>mean_var <span class="op">=</span> np.mean(errors_var)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>std_var <span class="op">=</span> np.std(errors_var)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame([</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    {<span class="st">&#39;Mean&#39;</span>: mean_same, <span class="st">&#39;Std. Dev.&#39;</span>: std_same},</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    {<span class="st">&#39;Mean&#39;</span>: mean_var, <span class="st">&#39;Std. Dev.&#39;</span>: std_var}]).rename({<span class="st">&#39;A&#39;</span>: <span class="st">&#39;Mean&#39;</span>, <span class="st">&#39;B&#39;</span>: <span class="st">&#39;Std. Dev.&#39;</span>}).rename({<span class="dv">0</span>: <span class="st">&quot;Same Dataset&quot;</span>, <span class="dv">1</span>: <span class="st">&quot;Varying Dataset&quot;</span>})</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>100%|██████████| 2/2 [00:37&lt;00:00, 18.70s/it]
100%|██████████| 2/2 [00:47&lt;00:00, 23.96s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>                     Mean  Std. Dev.
Same Dataset     0.009303   0.002800
Varying Dataset  0.006200   0.000208
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Against our intuition the same dataset is on average better in terms
of mean and std. dev. This tells us, that the specific dataset we got
here is not very good and on average a dataset is better suited for
training.</p>
</div>
<div class="cell markdown">
<div style="color: green; font-weight:
bold"> Comments:  

1. Including plots for each training iteration, similar to the sample's approach, would provide a more intuitive understanding of the model's performance.<br>  

2.Above code observes better performance on average with varying datasets than with the same dataset.This could indicate that the specific dataset used for the repeated training might not be very representative or that the model is robust to variations in the input data.<br>  

3.Investigating why the model performs differently on varying datasets compared to a constant dataset could uncover more insights about the model's learning capabilities.</div>
</div>
<section id="histogram-gmm-fitting-sampling" class="cell markdown">
<h2>Histogram, GMM fitting, sampling</h2>
</section>
<div class="cell code" data-execution_count="9">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>training_codes <span class="op">=</span> best_model_moons.encode(X_train_moon)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>plt.hist(training_codes.detach().numpy())</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Code Distribution for Two Moons&quot;</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>gmm <span class="op">=</span> GaussianMixture(<span class="dv">5</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>gmm.fit(training_codes.detach())</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>generated_codes, _ <span class="op">=</span> gmm.sample(<span class="dv">200</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>generated_codes <span class="op">=</span> torch.from_numpy(generated_codes).<span class="bu">type</span>(torch.float32)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>generated_points <span class="op">=</span> best_model_moons.decode(generated_codes).detach().numpy()</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>original <span class="op">=</span> plt.scatter(X_train_moon[:, <span class="dv">0</span>], X_train_moon[:, <span class="dv">1</span>])</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>gen <span class="op">=</span> plt.scatter(generated_points[:, <span class="dv">0</span>], generated_points[:, <span class="dv">1</span>])</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>plt.legend([original, gen], [<span class="st">&quot;Training Points&quot;</span>, <span class="st">&quot;Generated Points&quot;</span>])</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Generated Points with GMM fit to code distr.&quot;</span>)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/386704cacca5d1cab7a6c550bc47a0327f9bf7c0.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/ea916254a4da1d5cfc1320a8cb0086d929a0f930.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>The quality is good, except for some points. The original data has
noise, so the original points do not follow one line, but are
distributed around the center line of the moons. The autoencoder only
has a bottleneck of one, so this creates the codes only on a line. With
this all of the decoded codes must follow a line in some way. The more
powerful the model is, the more detailed the line the generated data
follows becomes. For a very overpowered model, it could probably overfit
and interpolate all points perfectly. The line the data follows thus
would follow a zig zag pattern through the main moons to hit as many
points as possible.</p>
<p>Also there exists a connection between the two moons in the center.
As the codes only have one connection, the generating line has no gaps,
only different slopes for the code distribution. This means there are
few codes between the moons and the most are clustered inside the
moons.</p>
<p>The GMM is probably able to spot the position between the oons and
tends to avoid it.</p>
</div>
<div class="cell markdown">
<div style="color: green; font-weight:
bold"> <h1> Comments:  </h1>
<ol>
<li>Experimenting with different numbers of components in the GMM could provide insights into how well the model can capture the data distribution.</li>

<li>Exploring how changes in the autoencoder's complexity (like increasing the bottleneck size) affect the generated data could be insightful.  </li>

<li>The understanding that the GMM helps in avoiding the generation of points in less dense areas between the two moons is astute.This shows an appreciation for how the GMM's components can capture the underlying structure of the data.Good implementation of GMM.</div>
</li>
</ol>
</div>
<section id="model-testing-with-higher-noise-without-retraining"
class="cell markdown">
<h2>Model Testing with higher noise without retraining</h2>
</section>
<div class="cell code" data-execution_count="10">
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># check on noise 0.2</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>X_test_moon_noise, _ <span class="op">=</span> make_moons(<span class="dv">10000</span>, noise<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>X_test_moon_noise <span class="op">=</span> torch.from_numpy(X_test_moon_noise).<span class="bu">type</span>(torch.float32)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>reconstructed_noisy_points <span class="op">=</span> best_model_moons(X_test_moon_noise).detach()</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>original <span class="op">=</span> plt.scatter(X_test_moon_noise[:, <span class="dv">0</span>], X_test_moon_noise[:, <span class="dv">1</span>])</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>gen <span class="op">=</span> plt.scatter(reconstructed_noisy_points[:, <span class="dv">0</span>], reconstructed_noisy_points[:, <span class="dv">1</span>])</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>plt.legend([original, gen], [<span class="st">&quot;Noisy Points&quot;</span>, <span class="st">&quot;Reconstruction of Noisy Points&quot;</span>])</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Reconstruction of points more noisy than training data&quot;</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/c3378ffa19cbe11a920e0afba19db4c8645700bf.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>The reconstruction seems still to behave fairly good, the codes are
in the area of a useful decoding.</p>
</div>
<div class="cell markdown">
<div style="color: green; font-weight:
bold">
<h1> Comments:  </h1>
<ol>
<li>Should consider applying the same scaling transformation to the noisy test data as was applied to the training data. This ensures a consistent basis for evaluating the autoencoder's performance.As here no scaling is applied to the noisy data.<br>  </li>

<li>Incorporating quantitative metrics, such as reconstruction error, could provide a more objective assessment of the autoencoder's performance on noisy data.<br></li></ol>
</div>
<section id="retraining-with-more-noise" class="cell markdown">
<h2>Retraining with more noise</h2>
</section>
<div class="cell code" data-execution_count="11">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># autoencoder with noisier training set</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>X_train_moon_noise, _ <span class="op">=</span> make_moons(<span class="dv">1000</span>, noise<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>X_train_moon_noise <span class="op">=</span> torch.from_numpy(X_train_moon_noise).<span class="bu">type</span>(torch.float32)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>model_noisy_training <span class="op">=</span> AutoEncoder(<span class="dv">2</span>, <span class="dv">1</span>, best_hidden_size, best_layer_count)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>train_autoencoder(model_noisy_training, X_train_moon_noise[:best_training_size], best_epochs, best_lr)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>reconstructed_noisy_points <span class="op">=</span> model_noisy_training(X_train_moon_noise).detach()</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>original <span class="op">=</span> plt.scatter(X_train_moon_noise[:, <span class="dv">0</span>], X_train_moon_noise[:, <span class="dv">1</span>])</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>gen <span class="op">=</span> plt.scatter(reconstructed_noisy_points[:, <span class="dv">0</span>], reconstructed_noisy_points[:, <span class="dv">1</span>])</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>plt.legend([original, gen], [<span class="st">&quot;Noisy Points&quot;</span>, <span class="st">&quot;Reconstruction of Noisy Points&quot;</span>])</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Reconstruction of noisy points and noisy training data&quot;</span>)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/2e0fa6bfadd27bd9326d42419518caf03737037c.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>Here we now see the line doint a lot of turns and does not respect
the space between the two moons, as this reduced the error while
training. Doesn't look like the moons tho</p>
</div>
<div class="cell markdown">
<div style="color: green; font-weight:
bold">
<h1>Comment:  </h1>
<ol>
<li>With noisier data, the reconstruction becomes more complex and less representative of the original clean data structure.  </li>

<li>Use  advanced techniques like the sample's use of MMD or other regularization methods could help in achieving a better balance between noise adaptation and maintaining the original data structure.</li></ol>
</div>
<section id="2-no-bottleneck" class="cell markdown">
<h1>2 No bottleneck</h1>
</section>
<div class="cell code" data-execution_count="12">
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> squared_exponential_kernel(x1, x2, h<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.exp(<span class="op">-</span>torch.<span class="bu">sum</span>((x1 <span class="op">-</span> x2) <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> h))</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> complete_mmd(X, Y, h<span class="op">=</span><span class="dv">1</span>, num_kernels<span class="op">=</span><span class="dv">7</span>):</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> Y.shape[<span class="dv">0</span>]</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    XX_raw <span class="op">=</span> torch.<span class="bu">sum</span>(torch.square(X[:, <span class="va">None</span>] <span class="op">-</span> X), axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    XX <span class="op">=</span> XX_raw[XX_raw <span class="op">!=</span> <span class="dv">0</span>]</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    YY_raw <span class="op">=</span> torch.<span class="bu">sum</span>(torch.square(Y[:, <span class="va">None</span>] <span class="op">-</span> Y), axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    YY <span class="op">=</span> YY_raw[YY_raw <span class="op">!=</span> <span class="dv">0</span>]</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    XY <span class="op">=</span> torch.<span class="bu">sum</span>(torch.square(X[:, <span class="va">None</span>] <span class="op">-</span> Y), axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>    current_h <span class="op">=</span> h</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    resultsXX <span class="op">=</span> torch.zeros(num_kernels)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>    resultsXX[<span class="dv">0</span>] <span class="op">=</span> torch.<span class="bu">sum</span>(torch.exp(<span class="op">-</span>XX <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> current_h)))</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    resultsYY <span class="op">=</span> torch.zeros(num_kernels)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>    resultsYY[<span class="dv">0</span>] <span class="op">=</span> torch.<span class="bu">sum</span>(torch.exp(<span class="op">-</span>YY <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> current_h)))</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    resultsXY <span class="op">=</span> torch.zeros(num_kernels)</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>    resultsXY[<span class="dv">0</span>] <span class="op">=</span> torch.<span class="bu">sum</span>(torch.exp(<span class="op">-</span>XY <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> current_h)))</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_kernels):</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>        current_h <span class="op">*=</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span></span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>        resultsYY[i] <span class="op">=</span> torch.<span class="bu">sum</span>(torch.exp(<span class="op">-</span>YY <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> current_h)))</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>        resultsXY[i] <span class="op">=</span> torch.<span class="bu">sum</span>(torch.exp(<span class="op">-</span>XY <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> current_h)))</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>        resultsXX[i] <span class="op">=</span> torch.<span class="bu">sum</span>(torch.exp(<span class="op">-</span>XX <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> current_h)))</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (N <span class="op">*</span> (N<span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> torch.<span class="bu">sum</span>(resultsXX) <span class="op">-</span> <span class="dv">2</span> <span class="op">/</span> (M <span class="op">*</span> N) <span class="op">*</span> torch.<span class="bu">sum</span>(resultsXY) <span class="op">+</span> <span class="dv">1</span> <span class="op">/</span> (M <span class="op">*</span> (M <span class="op">-</span> <span class="dv">1</span>)) <span class="op">*</span> torch.<span class="bu">sum</span>(resultsYY)</span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mmd_reconstruction_loss(X_true, X_gen, Z_true, Z_gen, h, num_kernels, l):</span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="dv">1</span> <span class="op">-</span> l) <span class="op">*</span> torch.nn.MSELoss()(X_true, X_gen) <span class="op">+</span> l <span class="op">*</span> complete_mmd(Z_true, Z_gen, h, num_kernels)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="13">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># hyperparameters: training set size, number epochs, learning rate</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_mmd_autoencoder(model, X_train, epochs<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">0.001</span>, l<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    optim <span class="op">=</span> torch.optim.Adam(model.parameters(), lr)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        codes <span class="op">=</span> model.encode(X_train)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model.decode(codes)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> mmd_reconstruction_loss(X_train, output, torch.randn_like(codes), codes, <span class="dv">1</span>, <span class="dv">3</span>, l)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        model.zero_grad()</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        error.backward()</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>        optim.step()</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="16">
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>model_mmd <span class="op">=</span> AutoEncoder(<span class="dv">2</span>, <span class="dv">2</span>, best_hidden_size, best_layer_count)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>train_mmd_autoencoder(model_mmd, X_train_moon[:<span class="dv">2000</span>], best_epochs, best_lr, <span class="fl">0.999</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>training_codes <span class="op">=</span> model_mmd.encode(X_train_moon).detach()</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>reconstructed_points <span class="op">=</span> model_mmd.decode(training_codes).detach()</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>original <span class="op">=</span> plt.scatter(X_train_moon[:, <span class="dv">0</span>], X_train_moon[:, <span class="dv">1</span>])</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>gen <span class="op">=</span> plt.scatter(reconstructed_points[:, <span class="dv">0</span>], reconstructed_points[:, <span class="dv">1</span>])</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>plt.legend([original, gen], [<span class="st">&quot;Original Points&quot;</span>, <span class="st">&quot;Reconstruction of Points&quot;</span>])</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Training points and reconstruction&quot;</span>)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>plt.scatter(training_codes[:, <span class="dv">0</span>], training_codes[:, <span class="dv">1</span>])</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Code Distribution of Training Data&quot;</span>)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>sampled_codes <span class="op">=</span> torch.randn((<span class="dv">500</span>, <span class="dv">2</span>))</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>generated_points <span class="op">=</span> model_mmd.decode(sampled_codes).detach()</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>original <span class="op">=</span> plt.scatter(X_train_moon[:, <span class="dv">0</span>], X_train_moon[:, <span class="dv">1</span>])</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>gen <span class="op">=</span> plt.scatter(generated_points[:, <span class="dv">0</span>], generated_points[:, <span class="dv">1</span>])</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>plt.legend([original, gen], [<span class="st">&quot;Original Points&quot;</span>, <span class="st">&quot;Generated Points&quot;</span>])</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Sampled Points with gaussian noise&quot;</span>)</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/015b7966d6e64eb13745e4651cc75f25894f9db1.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/af80f3a6edd6c6408ad6396a8e2a66f387efbc66.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/ad2baf78acc202cceca0c8bd7faff204610c6aba.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>The reconstruction seems to work up to some degree, but is shifted.
However as we have a bottleneck of two, the codes are not restricted as
a line, but can be placed in 2D.</p>
<p>Although we compared the output of the MMD function to other
implementations with same outputs, the codes do not behave like a
standard normal distribution. Maybe we selected the lambda not correctly
for this case or the bandwidth.</p>
<p>The generated points are in a correct interval (more or less), but do
not look too good. This was to be expected as the codes do not behave
like a standard normal.</p>
</div>
<div class="cell markdown">
<div style="color: green; font-weight:
bold"><h1>Comments:</h1>  
<ol>
<li>The  implementation the MMD loss using a series of exponential kernels is slightly different from the sample's, who uses inverse multi-quadratic kernels.  </li>

<li>.Consider using a ResNet architecture, as in the sample's code, which might offer better performance for this complex dataset.<br>  </li>

<li>.Using quantitative metrics like KL divergence to measure how closely the code distribution matches a standard normal distribution.<br>  </li>

<li>Experiment with different values of lambda and bandwidths in the MMD loss to find a more optimal balance between the reconstruction error and the standard normal distribution in the latent space.<br></li></div>
</div>
<section id="3-higher-dimensional-data" class="cell markdown">
<h1>3 Higher-dimensional data</h1>
</section>
<div class="cell code" data-execution_count="32">
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_digits</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>digit_dataset <span class="op">=</span> load_digits()</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> digit_dataset[<span class="st">&#39;data&#39;</span>]</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> digit_dataset[<span class="st">&#39;target&#39;</span>]</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>permutation <span class="op">=</span> torch.randperm(X.shape[<span class="dv">0</span>])</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.from_numpy(X).<span class="bu">type</span>(torch.float32)[permutation]</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> torch.from_numpy(Y).<span class="bu">type</span>(torch.float32)[permutation]</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>X_train_digit <span class="op">=</span> X[:(X.shape[<span class="dv">0</span>]<span class="op">*</span><span class="dv">2</span>) <span class="op">//</span> <span class="dv">3</span>]</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>Y_train_digit <span class="op">=</span> Y[:(Y.shape[<span class="dv">0</span>]<span class="op">*</span><span class="dv">2</span>) <span class="op">//</span> <span class="dv">3</span>]</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>X_test_digit <span class="op">=</span> X[(X.shape[<span class="dv">0</span>]<span class="op">*</span><span class="dv">2</span>) <span class="op">//</span> <span class="dv">3</span>:]</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>Y_test_digit <span class="op">=</span> Y[(Y.shape[<span class="dv">0</span>]<span class="op">*</span><span class="dv">2</span>) <span class="op">//</span> <span class="dv">3</span>:]</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="33">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>training_parameters <span class="op">=</span> [</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">300</span>, <span class="dv">500</span>, <span class="dv">800</span>, <span class="dv">1000</span>],</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">300</span>, <span class="dv">500</span>, <span class="dv">1000</span>],</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.003</span>, <span class="fl">0.002</span>, <span class="fl">0.001</span>, <span class="fl">0.0005</span>]</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_hyperparameters(X_train, X_test, bottleneck_size, hidden_size, layers, training_parameters):</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    N, input_size <span class="op">=</span> X_train.shape</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create data array</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    errors <span class="op">=</span> np.zeros((<span class="bu">len</span>(training_parameters[<span class="dv">0</span>]), <span class="bu">len</span>(training_parameters[<span class="dv">1</span>]), <span class="bu">len</span>(training_parameters[<span class="dv">2</span>])))</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, s <span class="kw">in</span> tqdm(<span class="bu">enumerate</span>(training_parameters[<span class="dv">0</span>]), total<span class="op">=</span><span class="bu">len</span>(training_parameters[<span class="dv">0</span>])):</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j, e <span class="kw">in</span> <span class="bu">enumerate</span>(training_parameters[<span class="dv">1</span>]):</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k, l <span class="kw">in</span> <span class="bu">enumerate</span>(training_parameters[<span class="dv">2</span>]):</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>                model <span class="op">=</span> AutoEncoder(input_size, bottleneck_size, hidden_size, layers)</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>                train_autoencoder(model, X_train[:s], e, l)</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>                output <span class="op">=</span> model(X_test)</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>                errors[i, j, k] <span class="op">=</span> torch.nn.MSELoss()(X_test, output)</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> errors</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="34">
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>errors <span class="op">=</span> test_hyperparameters(X_train_digit, X_test_digit, <span class="dv">2</span>, <span class="dv">65</span>, <span class="dv">1</span>, training_parameters)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>best_training_size_digits <span class="op">=</span> training_parameters[<span class="dv">0</span>][np.argmin(np.<span class="bu">min</span>(errors, axis<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>)))]</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>best_epochs_digits <span class="op">=</span> training_parameters[<span class="dv">1</span>][np.argmin(np.<span class="bu">min</span>(errors, axis<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">2</span>)))]</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>best_lr_digits <span class="op">=</span> training_parameters[<span class="dv">2</span>][np.argmin(np.<span class="bu">min</span>(errors, axis<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>)))]</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;best combination: size = </span><span class="sc">{</span>best_training_size_digits<span class="sc">}</span><span class="ss">, epochs = </span><span class="sc">{</span>best_epochs_digits<span class="sc">}</span><span class="ss">, lr = </span><span class="sc">{</span>best_lr_digits<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(possible_parameters):</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    plot_min_for_parameter(training_parameters[i], errors, p)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>100%|██████████| 4/4 [01:40&lt;00:00, 25.19s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>best combination: size = 1000, epochs = 1000, lr = 0.002
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/02a26ce585b48112fc40c7217f6e3003561d912f.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/c9d1302afd6f86b5bca66ad676feb227d514ea2e.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/0daf11ee8751f207ac0f426ff82518b358904d92.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="41">
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>hidden_sizes <span class="op">=</span> [<span class="dv">65</span>, <span class="dv">70</span>, <span class="dv">80</span>, <span class="dv">100</span>]</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>layer_sizes <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_network_structure(X_train, X_test, bottleneck_size, hidden_sizes, layer_sizes, epochs, lr):</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    N, input_size <span class="op">=</span> X_train.shape</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create data array</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    errors <span class="op">=</span> np.zeros((<span class="bu">len</span>(hidden_sizes), <span class="bu">len</span>(layer_sizes)))</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, h <span class="kw">in</span> tqdm(<span class="bu">enumerate</span>(hidden_sizes), total<span class="op">=</span><span class="bu">len</span>(hidden_sizes)):</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j, l <span class="kw">in</span> <span class="bu">enumerate</span>(layer_sizes):</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> AutoEncoder(input_size, bottleneck_size, h, l)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>            train_autoencoder(model, X_train[:N], epochs, lr)</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> model(X_test)</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>            errors[i, j] <span class="op">=</span> torch.nn.MSELoss()(X_test, output)</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> errors</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="42">
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>errors <span class="op">=</span> test_network_structure(X_train_digit, X_test_digit, <span class="dv">2</span>, hidden_sizes, layer_sizes, best_epochs_digits, best_lr_digits)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>best_hidden_size_digits <span class="op">=</span> hidden_sizes[np.argmin(np.<span class="bu">min</span>(errors, axis<span class="op">=</span><span class="dv">1</span>))]</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>best_layer_count_digits <span class="op">=</span> layer_sizes[np.argmin(np.<span class="bu">min</span>(errors, axis<span class="op">=</span><span class="dv">0</span>))]</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;best network: hidden: </span><span class="sc">{</span>best_hidden_size_digits<span class="sc">}</span><span class="ss">, layer count: </span><span class="sc">{</span>best_layer_count_digits<span class="sc">}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>100%|██████████| 4/4 [01:47&lt;00:00, 26.81s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>best network: hidden: 100, layer count: 2
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="43">
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>bottleneck_sizes <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>]</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_bottleneck(X_train, X_test, bottleneck_sizes, hidden_size, layer_count, epochs, lr):</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    N, input_size <span class="op">=</span> X_train.shape</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create data array</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    errors <span class="op">=</span> np.zeros((<span class="bu">len</span>(bottleneck_sizes)))</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    reconstructed <span class="op">=</span> np.zeros((<span class="bu">len</span>(bottleneck_sizes), <span class="dv">10</span>, <span class="dv">8</span><span class="op">*</span><span class="dv">8</span>))</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, bn <span class="kw">in</span> tqdm(<span class="bu">enumerate</span>(bottleneck_sizes), total<span class="op">=</span><span class="bu">len</span>(bottleneck_sizes)):</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> AutoEncoder(input_size, bn, hidden_size, layer_count)</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>            train_autoencoder(model, X_train, epochs, lr)</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> model(X_test)</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>            reconstructed[i] <span class="op">=</span> output[:<span class="dv">10</span>].detach().numpy()</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>            errors[i] <span class="op">=</span> torch.nn.MSELoss()(X_test, output)</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> errors, reconstructed</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_test_digit.shape)</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>errors, reconstructed <span class="op">=</span> test_bottleneck(X_train_digit[:best_training_size_digits], X_test_digit, bottleneck_sizes, best_hidden_size_digits, best_layer_count_digits, best_epochs_digits, best_lr_digits)</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>best_bottleneck_size_digits <span class="op">=</span> bottleneck_sizes[np.argmin(errors)]</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;best bottleneck: </span><span class="sc">{</span>best_bottleneck_size_digits<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>reconstructed <span class="op">=</span> reconstructed.reshape(<span class="dv">3</span>, <span class="dv">10</span>, <span class="dv">8</span>, <span class="dv">8</span>).swapaxes(<span class="dv">1</span>, <span class="dv">2</span>).reshape(<span class="dv">3</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">10</span><span class="op">*</span><span class="dv">8</span>)</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>plt.imshow(np.vstack([X_train_digit[:<span class="dv">10</span>].reshape(<span class="dv">10</span>, <span class="dv">8</span>, <span class="dv">8</span>).swapaxes(<span class="dv">0</span>, <span class="dv">1</span>).reshape(<span class="dv">8</span>, <span class="dv">8</span><span class="op">*</span><span class="dv">10</span>), reconstructed]))</span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Digit Comparison - Original, Bottlenecks 2, 4, 8&quot;</span>)</span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;Samples&quot;</span>)</span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Top to bottom: original, 2, 4, 8&quot;</span>)</span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a>plot_line(<span class="st">&quot;Errors for differing bottleneck sizes&quot;</span>, bottleneck_sizes, errors, <span class="st">&quot;Bottleneck size&quot;</span>, <span class="st">&quot;Error&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>torch.Size([599, 64])
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100%|██████████| 3/3 [00:18&lt;00:00,  6.06s/it]</code></pre>
</div>
<div class="output stream stdout">
<pre><code>best bottleneck: 8
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/ecd272ac2886bdbbb94554d86eb1f4c68b89a653.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/b58a31208a3a4d2dc50f2236369464ef484474bf.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>The numbers are not correct, however the results look somewhat like
convincing numbers. But they changed the value for all bottlenecks.
However we had runs, where the results were better than this. No
time</p>
<h3 id="bottleneck-2">Bottleneck 2</h3>
<p>This one is the most blurry.</p>
<h3 id="bottleneck-4">Bottleneck 4</h3>
<p>The numbers here are the most balanced, but still incorrect.</p>
<h3 id="bottleneck-8">Bottleneck 8</h3>
<p>The numbers are very similar to bottleneck size 4.</p>
</div>
<div class="cell code" data-execution_count="44">
<div class="sourceCode" id="cb40"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> RandomForestClassifier()</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>classifier.fit(X_train_digit, Y_train_digit)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>model_digits <span class="op">=</span> AutoEncoder(<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, best_bottleneck_size_digits, best_hidden_size_digits, best_layer_count_digits)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>train_autoencoder(model_digits, X_train_digit, best_epochs_digits, best_lr_digits)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>X_test_digit_reconstructed <span class="op">=</span> model_digits(X_test_digit)</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="co"># classify both sets</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>predictions_true <span class="op">=</span> classifier.predict(X_test_digit.detach())</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>predictions_reconst <span class="op">=</span> classifier.predict(X_test_digit_reconstructed.detach())</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>same_predictions <span class="op">=</span> predictions_true <span class="op">==</span> predictions_reconst</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;The labels match with </span><span class="sc">{</span>(np.mean(same_predictions) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:.2f}</span><span class="ss">%&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The labels match with 92.15%
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="45">
<div class="sourceCode" id="cb42"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>model_digits_two <span class="op">=</span> AutoEncoder(<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">2</span>, best_hidden_size_digits, best_layer_count_digits)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>train_autoencoder(model_digits_two, X_train_digit, best_epochs_digits, best_lr_digits)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>digit_codes <span class="op">=</span> model_digits_two.encode(X_train_digit).detach()</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>plots <span class="op">=</span> []</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>names <span class="op">=</span> []</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>    relevant_codes <span class="op">=</span> digit_codes[Y_train_digit <span class="op">==</span> i]</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    plot <span class="op">=</span> plt.scatter(relevant_codes[:, <span class="dv">0</span>], relevant_codes[:, <span class="dv">1</span>])</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>    plots.append(plot)</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>    names.append(<span class="ss">f&#39;</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>plt.legend(plots, names)</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Distribution of Different Labels&quot;</span>)</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/adc70c339dea41ab1dedc4a490e75b9828ca7ca8.png" /></p>
</div>
</div>
<div class="cell markdown">
<p>We can see, that the codes of the same numbers are clustered together
mostly. There are some outliers.</p>
<p>There are some clusters overlapping, for example on the right the
light blue and red clusters overlap. These are 3 and 9. If one closes
the upper part of the 3 it looks like a nine so this makes sense. Also
some numbers have multiple clusters located in different parts: The 1
seems to have two different versions, one between 4 and 7. These are
probably the ones very sharp. Also there are ones overlapping with 8s.
If an 8 is very thin, it almost looks like a 1 with this resolution.</p>
<p>The numbers are more or less centered around a point. It seems like
the clusters are arranged circular around a common center. Also we have
4 on one side and 3 with 9 on the other, probably we have a sorting of
"sharp" and "round" numbers. 0 is also there so maybe its roundness.</p>
</div>
<div class="cell code" data-execution_count="46">
<div class="sourceCode" id="cb43"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gmm_plot_classify_bottleneck(X_train, X_test, bottleneck_sizes, hidden_size, layer_count, epochs, lr, classifier):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    N, input_size <span class="op">=</span> X_train.shape</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create data array</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    errors <span class="op">=</span> np.zeros((<span class="bu">len</span>(bottleneck_sizes)))</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, bn <span class="kw">in</span> <span class="bu">enumerate</span>(bottleneck_sizes):</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>            <span class="co"># train the model</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> AutoEncoder(input_size, bn, hidden_size, layer_count)</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>            train_autoencoder(model, X_train, epochs, lr)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># fit the gmm</span></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>            training_codes <span class="op">=</span> model.encode(X_train).detach()</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>            gmm <span class="op">=</span> GaussianMixture(<span class="dv">5</span>)</span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>            gmm.fit(training_codes.detach())</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># generate synthetic data</span></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>            num_gen <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>            generated_codes, _ <span class="op">=</span> gmm.sample(num_gen)</span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>            generated_codes <span class="op">=</span> torch.from_numpy(generated_codes).<span class="bu">type</span>(torch.float32)</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>            generated_digits <span class="op">=</span> model.decode(generated_codes).detach().numpy()</span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>            predictions_gen <span class="op">=</span> classifier.predict(generated_digits)</span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>            <span class="co"># plot the images</span></span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a>            reconstructed <span class="op">=</span> generated_digits</span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>            plt.imshow(reconstructed.reshape(num_gen, <span class="dv">8</span>, <span class="dv">8</span>).swapaxes(<span class="dv">0</span>, <span class="dv">1</span>).reshape(<span class="dv">8</span>, <span class="dv">8</span><span class="op">*</span>num_gen))</span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>            plt.title(<span class="ss">f&quot;Samples for bottleneck = </span><span class="sc">{</span>bn<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a>            plt.xlabel(<span class="ss">f&quot;Predicted Labels: </span><span class="sc">{</span><span class="st">&#39;, &#39;</span><span class="sc">.</span>join([<span class="bu">str</span>(<span class="bu">int</span>(v)) <span class="cf">for</span> v <span class="kw">in</span> predictions_gen])<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a>            plt.show()</span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-37"><a href="#cb43-37" aria-hidden="true" tabindex="-1"></a>gmm_plot_classify_bottleneck(X_train_digit, X_test_digit, bottleneck_sizes, best_hidden_size_digits, best_layer_count_digits, best_epochs_digits, best_lr_digits, classifier)</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/eb3b36e0d0cb72d2ec43cf635830feabade05743.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/5ee54c1f395f4f8d44b46b905e467e8063f57f87.png" /></p>
</div>
<div class="output display_data">
<p><img
src="vertopal_e4699dfd20d5448eb6cfe8601401f58b/8c0fae71d4bad220171ee806ef7c8567d05a8925.png" /></p>
</div>
</div>
<div class="cell markdown">
<h3 id="bottleneck-2">Bottleneck 2</h3>
<p>The numbers are very clear, even the blurries on the right can be
recognized as a 9 or 5, but not clearly. The classifier is very close to
our interpretation.</p>
<h3 id="bottleneck-4">Bottleneck 4</h3>
<p>The background of the numbers is not as pronounced as for bottleneck
2. The numbers are still good. The classifier is also close to our
prediction, except for the 4th sample that could be a 9 or 7.</p>
<h3 id="bottleneck-8">Bottleneck 8</h3>
<p>The background is not as good as for size 2, but better than for size
4. The numbers still look mostly good, but not as good as the others.
The numbers have lower quality in our opinion. The classifier mostly
agrees with our predictions except for the 9th number, we would predict
it to be a 8 or maybe a bad 3, but not a 9.</p>
</div>
<div class="cell markdown">
<div style="color: green; font-weight:
bold"><h1>Comments:  </h1>
<ol>
<li>The approach to generating and evaluating synthetic digits using GMMs adds an extra dimension to the analysis, especially in assessing the quality and classifiability of these synthetic digits.Good job. <br>  </li>

<li>A deeper analysis of the classifier's performance, including confusion matrices, could provide more detailed insights into which digits are more prone to misclassification after reconstruction.</li> </div>
</div>
</body>
</html>
